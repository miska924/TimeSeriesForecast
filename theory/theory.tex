\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{graphicx}
\usepackage{amsmath, amssymb, mathtools}
\usepackage{amsthm}
\usepackage[T2A]{fontenc}
\usepackage{enumitem}
\usepackage[margin=0.6in]{geometry}
\usepackage[usenames]{color}
\usepackage{colortbl}
\linespread{1.1}

\usepackage{cellspace}
\usepackage[linkcolor=blue,colorlinks=true]{hyperref}

\usepackage{mleftright,xparse}

\usepackage{etoolbox}
\apptocmd{\lim}{\limits}{}{}
\apptocmd{\sum}{\limits}{}{}
\apptocmd{\prod}{\limits}{}{}
\apptocmd{\inf}{\limits}{}{}
\apptocmd{\int}{\limits}{}{}


\NewDocumentCommand\xDeclarePairedDelimiter{mmm}
{%
 \NewDocumentCommand#1{som}{%
  \IfNoValueTF{##2}
   {\IfBooleanTF{##1}{#2##3#3}{\mleft#2##3\mright#3}}
   {\mathopen{##2#2}##3\mathclose{##2#3}}%
 }%
}

\let\originaleqref\eqref % or \let\originaleqref\ref to drop parens
\makeatletter
\renewcommand{\eqref}[1]{%
  \begingroup%
  \let\ref\@refstar%
  \hyperref[#1]{\originaleqref{#1}}%
  \endgroup
}
\makeatother

\DeclareMathOperator{\rng}{rng}
\DeclareMathOperator{\comb}{C}
\DeclareMathOperator{\mexp}{\mathbb{E}}
\DeclareMathOperator{\var}{\mathbb{D}}
\DeclareMathOperator{\prob}{\mathsf{P}}
\renewcommand{\geq}{\geqslant}
\renewcommand{\leq}{\leqslant}
\DeclareMathOperator{\isgeq}{\overset{?}{\geq}}
\DeclareMathOperator{\isgt}{\overset{?}{>}}
\DeclareMathOperator{\isleq}{\overset{?}{\leq}}
\DeclareMathOperator{\islt}{\overset{?}{<}}
\DeclareMathOperator{\RR}{\mathbb{R}}
\DeclareMathOperator{\NN}{\mathbb{N}}
\DeclareMathOperator{\QQ}{\mathbb{Q}}
\DeclareMathOperator{\ZZ}{\mathbb{Z}}
\DeclareMathOperator{\CC}{\mathbb{C}}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\Pois}{Pois}
\DeclareMathOperator{\Norm}{\mathcal{N}}
\DeclareMathOperator{\Fshr}{F}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\Beta}{B}

\xDeclarePairedDelimiter{\abs}{\lvert}{\rvert}

\newtheorem{definition}{Определение}
\newtheorem*{definition*}{Определение}
\newtheorem{comment}{Замечание}
\newtheorem*{comment*}{Замечание}
\newtheorem{example}{Пример}
\newtheorem*{example*}{Пример}
\newtheorem{theorem}{Теорема}
\newtheorem*{theorem*}{Теорема}

\title {Теоретические материалы для проекта}
\author {Лоптев Сергей}

\begin{document}
    \maketitle     
    \tableofcontents
    \newpage
    
    \part{Теория вероятностей}
    \section{Дискретные распределения}
    \subsection{Вероятность}
    Пусть задано некоторое множество возможных исходов (эксперимента) $\Omega = \left\{\omega_1, \omega_2, \ldots, \omega_n\right\}$. Это множество $\Omega$ называют \textbf{множеством элементарных исходов}. Всякое подмножество $A \subset \Omega$ называют \textbf{событием}. Функцию $\prob : 2^{\Omega} \to \left[0, 1\right]$, удовлетворяющую следующим свойствам:
    \begin{enumerate}[label=(\roman*)]
        \item $\prob(\Omega) = 1$,
        \item $A \cup B = \varnothing \implies \prob(A \cup B) = \prob(A) + \prob(B)$ (правило суммы или аддитивность),
    \end{enumerate} 
    называют \textbf{вероятностной мерой}, а значение $\prob(A)$ \textbf{вероятностью} события $A$.
    \subsection{Распределения}
    \paragraph{Распределение Бернулли}
    Говорят, что случайная величина $I_A$ обладает биномиальным распределением, если для некоторого события $A$ и элементарного исхода $\omega$
    \begin{equation*}
        I_A(\omega) = \begin{dcases}
            1, &\omega \in A\\
            0, &\omega \notin A
        \end{dcases}
    \end{equation*}
    
    \paragraph{Биномиальное распределение}
    Говорят, что случайная величина $X$ обладает биномиальным распределением с параметром $p \in \left[0, 1\right]$, если для некоторого $N$
    \begin{equation*}
        \forall k \in \lbrace 0, 1, \ldots, N \rbrace\ \prob (X = n) = \comb_N^k p^k \left(1 - p\right)^{N-k}
    \end{equation*}
    
    \paragraph{Геометрическое распределение}
    Говорят, что случайная величина $X$ обладает геометрическим распределением с параметром $p \in \left[0, 1\right]$, если
    \begin{equation*}
        \forall n \in \NN\ \prob (X = n) = p\left(1 - p\right)^{n - 1}
    \end{equation*}
    
    \paragraph{Распределение Пуассона}
    Говорят, что случайная величина $X$ обладает распределением Пуассона с параметром $\lambda$ ($X \sim \Pois(\lambda)$), если
    \begin{equation*}
        \forall n \in \NN \cup \lbrace 0\rbrace\ \prob (X = n) = \frac{\lambda}{n!} \cdot e^{-\lambda}
    \end{equation*}
    
    \subsection{Совместные распределения случайных величин}
    Пусть $X, Y$~--- две случайные величины на дискретном вероятностном пространстве с множествами (различных) значений $\lbrace x_1, \ldots, x_k, \ldots \rbrace$ и $\lbrace y_1, \ldots, y_k, \ldots\rbrace$ соответственно. Их \textbf{совместным распределением} называется вероятностная мера $\mu_{\left(X,Y\right)}$ на вероятностном пространстве всех пар $\left(x_j, y_k\right)$, для которой
    \begin{equation*}
        \mu_{\left(X,Y\right)}\left(\lbrace \left(x_j, y_k\right)\rbrace\right) = \prob(\omega:\ X(\omega)=x_j,\ Y(\omega)=y_k) = \prob(\left\{\omega:\ X(\omega)=x_j\right\}\cap \left\{\omega:\ Y(\omega)=y_k\right\})
    \end{equation*}
    
    \subsection{Таблица совместного распределения и частное распределение}
    Совместное распределение двух случайных величин $X,Y$ с множествами значений $\lbrace x_1, \ldots, x_k, \ldots \rbrace$ и $\lbrace y_1, \ldots, y_k, \ldots\rbrace$ однозначно задаётся \textbf{таблицей совместного распределения}, у которой в ячейке $i,j$ стоит $\prob (\omega: X(\omega) = x_i,\ Y(\omega) = y_k)$. Из такой таблицы мы можем получить \textbf{частное распределение} случайной величины $X$: 
    \begin{equation*}
        P(\omega: X(\omega) = x_j) = \sum_{k=1}^{\infty} P(\omega: X(\omega) = x_j, Y(\omega) = y_k)    
    \end{equation*}
    Аналогично можно получить частное распределение случайной величины $Y.$

    \subsection{Независимость случайных величин и некореллированность случайных величин}
    Случайные величины $X, Y$ с множествами значений $\lbrace x_1, \ldots, x_k, \ldots \rbrace$ и $\lbrace y_1, \ldots, y_k, \ldots\rbrace$ соответственно называются \textbf{независимыми}, если
    \begin{equation*}
        \mu_{\left(X,Y\right)}\left(\lbrace \left(x_j, y_k\right)\rbrace\right) = \mu_X \left(\left\{x_j\right\}\right) \cdot \mu_Y \left(\left\{y_k\right\}\right)\hspace{1em} \forall k,j
    \end{equation*}
    или другими словами
    \begin{equation*}
        \prob(\omega:\ X(\omega) = x_j,\ Y(\omega) = y_k) = \prob(\omega:\ X(\omega) = x_j) \cdot \prob(\omega:\ Y(\omega) = y_k)\hspace{1em} \forall k,j
    \end{equation*}
    Аналогично определяется независимость трёх и более случайных величин.
    
    \textbf{Ковариацией} пары случайных величин называется число 
    \begin{equation*}
        \cov(X, Y) = \mexp\left[\left(X - \mexp X\right)\left(Y - \mexp Y\right)\right]
    \end{equation*}
    Заметим, что ковариация является неотрицательно определённой билинейной формой,
    \begin{equation*}
        \cov(X,Y) = \mexp\left[X \cdot Y\right] - \left[\mexp X\right] \cdot \left[\mexp Y\right]
    \end{equation*}
    в частности $\var X = \cov(X, X) = \mexp \left[X^2\right] - \left[\mexp X\right]^2$.

    Величину
    \begin{equation*}
        r(X, Y) = \frac{\cov(X, Y)}{\sqrt{\var X}\sqrt{\var Y}}
    \end{equation*}
    называют \textbf{коэффициентом коррелляции}.

    Две случайные величины $X,Y$ называются \textbf{некорреллированными}, если $r(X, Y) = 0$. 

    Заметим, что если две случайные величины $X, Y$ независимы, то они некорреллированы. Обратное, вообще говоря, неверно.

    \subsection{Моменты распределений}
    Число $\mexp \xi^k$ называется \textbf{моментом порядка $k$} или \textbf{$k$-ым моментом случайной величины $\xi$}. 

    \section{Непрерывные распределения}
    \subsection{Плотность распределения}
    Предположим, что определена вероятностная мера $\mu$. Функция
    \begin{equation*}
        F(t) = \mu(\left(-\infty, t\right])
    \end{equation*}
    называется \textbf{функцией распределения} меры $\mu$.

    Из определения $F$ следует, что $\mu(\left(a, b\right]) = F(b) - F(a)$.

    Функция $F$ удовлетворяет следующим свойствам: 
    \begin{enumerate}[label=(\roman*)]
        \item $F: \RR \to \left[0, 1\right]$ не убывает;
        \item $F$ непрерывна справа;
        \item $\lim_{t \to -\infty} F(t) = 0$ и $\lim_{t \to +\infty} F(t) = 1$.
    \end{enumerate}

    Во многих случаях задавать вероятностную меру на числовой прямой удобно плотностью.

    Пусть $\varrho$~--- неотрицательная и интегрируемая (по Риману) функция на $\RR$, причём
    \begin{equation*}
        \int_{-\infty}^{+\infty} \varrho(x) dx = 1
    \end{equation*}
    Если функция распределения $F$ меры $\mu$ задаётся неравенством
    \begin{equation*}
        F(t) = \int_{-\infty}^{t} \varrho(x) dx
    \end{equation*}
    то говорят, что вероятностная мера $\mu$ задана \textbf{плотностью} $\varrho$. В этом случае
    \begin{equation*}
        \mu(\left(a, b\right]) = F(b) - F(a) = \int_{a}^{b} \varrho(x)dx
    \end{equation*}
    На самом деле можно доказать, что
    \begin{equation*}
        \mu(A) = \int_A \varrho dx
    \end{equation*}
    для всякого множества $A$, для которого имеет смысл интеграл в правой части.

    \subsection{Распределения}
    \paragraph{Нормальное распределение}
    Нормальным распределением с параметрами $\mu$ и $\sigma^2$ называют вероятностную меру на числовой прямой, заданную плотностью
    \begin{equation*}
        \varrho(x) = \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{x - \mu}{2 \sigma^2}}
    \end{equation*}
    Случайную величину с нормальным распределением обозначают как $X \sim \mathcal{N}(\mu, \sigma^2)$.
    \begin{center}
        \includegraphics[width=10cm]{normal.png}
    \end{center}

    \begin{definition*}
        Введём понятие \textbf{Гамма-функции}.
        \begin{equation*}
            \Gamma(z) = \lim_{n \to +\infty} \frac{\left(n - 1\right)! n^z}{z(z+1)(z+2)\cdots(z + n - 1)},\hspace{1em} z \in \CC \setminus \left\{0,\ -1,\ -2, \ldots\right\}
        \end{equation*}        
    \end{definition*}

    \paragraph{Распределение Стьюдента}
    Пусть $Y_0,\ Y_1,\ \ldots,\ Y_n$~--- независимые стандартные нормальные случайные величины, такие что $Y_i \sim \mathcal{N}(0, 1),\ i = 0,\ldots,n$.
    Тогда распределение случайной величины $t$, где
    \begin{equation*}
        t={\frac {Y_{0}}{\sqrt {{\frac {1}{n}}\sum \limits _{i=1}^{n}Y_{i}^{2}}}},
    \end{equation*}
    называется \textbf{распределением Стьюдента} с $n$ степенями свободы $t \sim t(n)$.

    Это распределение абсолютно непрерывно с плотностью:
    \begin{equation*}
        \varrho_{t}(y)={\frac {\Gamma \left({\frac {n+1}{2}}\right)}{{\sqrt {n\pi }}\,\Gamma \left({\frac {n}{2}}\right)}}\,\left(1+{\frac {y^{2}}{n}}\right)^{-{\frac {n+1}{2}}},
    \end{equation*}
    где $\Gamma$~--- гамма-функция. Таким образом:
    \begin{equation*}
        \frac{\Gamma(\frac{ n+1}{2})} {\sqrt{ n\pi}\,\Gamma(\frac{ n}{2})} =  \frac{( n -1)( n -3)\cdots 5 \cdot 3} {2\sqrt{ n}( n -2)( n -4)\cdots 4 \cdot 2\,}, \text{ для чётных $n$}
    \end{equation*}
    и соответственно 
    \begin{equation*}
        {\frac {\Gamma ({\frac {n+1}{2}})}{{\sqrt {n\pi }}\,\Gamma ({\frac {n}{2}})}}={\frac {(n-1)(n-3)\cdots 4\cdot 2}{\pi {\sqrt {n}}(n-2)(n-4)\cdots 5\cdot 3\,}}, \text{ для нечётных $n$.}
    \end{equation*}
    \begin{center}
        \includegraphics[width=15cm]{student.png}
    \end{center}

    \paragraph{Распределение хи-квадрат} Пусть $z_1, \ldots, z_k$~--- совместно независимые стандартные нормальный случайные величины, то есть $z_i \sim \Norm(0, 1)$. Тогда случайная величина 
    \begin{equation*}
        x = z_1^2 + \ldots + z_k^2
    \end{equation*}
    имеет \textbf{распределение хи-квадрат} c $k$ степенями свободы, то есть $x \sim \varrho_{\chi^2(k)}(x)$ или, если записать по-другому:
    \begin{equation*}
        x = \sum\limits_{i=1}^k z_i^2 \sim \chi^2(k)
    \end{equation*}

    Плотность распределения хи-квадрат имеет вид:
    \begin{equation*}
        \varrho_{\chi^2(k)}(x) = \frac{(1/2)^{k \over 2}}{\Gamma\!\left({k \over 2}\right)}\, x^{{k \over 2} - 1}\, e^{-\frac{x}{2}}
    \end{equation*}
    \begin{center}
        \includegraphics[width=10cm]{chi-square.png}
    \end{center}

    \begin{definition*}
        Введём понятие \textbf{Бета-функции}. 
        
        Формула через Гамма-функции:
        \begin{equation*}
            \Beta(x,y) = \frac{\Gamma(x) \Gamma(y)}{\Gamma(x + y)},
        \end{equation*}     
        где $\Gamma(x)$~--- Гамма-функция.
        
        Формула через нисходящий факториал:
        \begin{equation*}
            \Beta(x,y) = \frac{1}{y} \sum_{n=0}^{\infty} (-1)^n \frac{\left(y\right)_{n+1}}{n!\left(x + n\right)},
        \end{equation*}
        где $\left(x\right)_n$~--- нисходящий факториал, равный $x \cdot (x - 1) \cdot (x - 2) \cdot \ldots \cdot (x - n + 1)$.
    \end{definition*}
    
    \paragraph{Распределение Фишера}
    Пусть $Y_1,\ Y_2$~--- две независимые случайные величины, имеющие распределение хи-квадрат: $Y_i \sim \chi^2(d_i)$, где $d_i \in \NN,\ i = 1, 2$.
    Тогда распределение случайной величины 
    \begin{equation*}
        F = \frac{Y_1/d_1}{Y_2/d_2}
    \end{equation*}
    называется \textbf{распределением Фишера} со степенями свободы $d_1$ и $d_2$. Пишут $F \sim \Fshr(d_1, d_2)$.

    Опишем теперь функцию плотности случайной величины $F$:
    \begin{equation*}
        \varrho_F(x) = \frac{\sqrt{\frac{(d_1\,x)^{d_1}\,\,d_2^{d_2}}
        {(d_1\,x+d_2)^{d_1+d_2}}}}
        {x\,\mathrm{B}\!\left(\frac{d_1}{2},\frac{d_2}{2}\right)}
    \end{equation*}

    Картинка:
    \begin{center}
        \includegraphics[width=10cm]{fisher.png}
    \end{center}

    \subsection{Совместное распределение случайных величин}
    Пусть $\xi$ и $\eta$~--- случайные величины. Рассмотрим отображение $\omega \to \left(\xi(\omega), \eta(\omega)\right)$. Это отображение определяет на плоскости вероятностную меру $\mu$ следующим образом:
    \begin{equation*}
        \mu(B) = \prob(\left\{\omega:\ \left(\xi(\omega), \eta(\omega)\right) \in B\right\})
    \end{equation*}
    Меру $\mu$ называют \textbf{совместным распределением} случайных величин $\xi$ и $\eta$. Функцию 
    \begin{equation*}
        F(x,y) = \mu(\left(-\infty, x\right] \times \left(-\infty, y\right]) = \prob(\left\{\omega:\ \xi(\omega) \leq x \text{ и } \eta(\omega) \leq y\right\})
    \end{equation*}
    называют \textbf{функцией совместного распределения} случайных величин $\xi$ и $\eta$.

    Аналогичным образом определяется совместное распределение любого конечного числа случайных величин.

    \subsection{Совместная и частная плотность}
    Если существует интегрируемая (по Риману) и неотрицательная функция $\varrho(x, y)$ такая, что 
    \begin{equation*}
        \mu(A) = \int \int_A \varrho(x, y) dx dy
    \end{equation*}
    для всякого допустимого (измеримого по Жордану) множества $A$, то $\varrho$ называется \textbf{совместной плотностью распределения}.

    Если известна плотность $\varrho$ совместного распределения $\xi$ и $\eta$, то можно найти плотности распределения каждой из случайных величин. Например, для случайной величины $\xi$:
    \begin{equation*}
        \mu_{\xi}(\left(a, b\right]) = \mu(\left(a, b\right] \times \RR) = \int_a^b \left(\int_{-\infty}^{+\infty} \varrho(x, y) dy\right) dx
    \end{equation*}
    и, следовательно,
    \begin{equation*}
        \varrho_\xi(x) = \int_{-\infty}^{+\infty} \varrho(x,y) dy
    \end{equation*}
    Если распределение каждой из случайных величин задаётся плотностью, то совместное распределение может не иметь плотность.

    \subsection{Независимые случайные величины и некореллированность случайных величин}
    Случайные величины $\xi$ и $\eta$ называются \textbf{независимыми}, если для всяких промежутков $U$ и $V$ выполняется равенство
    \begin{equation*}
        \prob(\left\{\omega:\ \xi(\omega) \in U \text{ и } \eta(\omega) \in V\right\}) = \prob(\left\{\omega:\ \xi(\omega) \in U\right\}) \cdot \prob(\left\{\omega:\ \eta(\omega) \in V\right\})
    \end{equation*}
    В терминах совместного распределения это равенство записывается так:
    \begin{equation*}
        \mu(U \times V = \mu_\xi(U) \cdot \mu_\eta(V)
    \end{equation*}
    Говорят, что мера $\mu$ является произведением мер $\mu_\xi$ и $\mu_\eta$ и пишут $\mu = \mu_\xi \otimes \mu_\eta$.

    Независимость $\xi$ и $\nu$ равносильна тому, что
    \begin{equation*}
        F(x, y) = F_\xi(x) \cdot F_\eta(y)
    \end{equation*}
    где $F$~--- совместная функция распределения $\xi$ и $\eta$.

    Для непрерывных случайных величин меняется определение математического ожидания. Математическое ожидание непрерывной случайной величины, распредление которой задаётся плотностью $\varrho(x)$, равно
    \begin{equation*}
        \mexp[\xi] = \int_{-\infty}^{+\infty} x \varrho(x) dx
    \end{equation*}
    Определения дисперсии, ковариации и коэффициента коррелляции требуют лишь дополнительного условия $\mexp \xi^2 < \infty$. Таким образом, определение некорреллированности случайных величин остаётся тем же: две случайные величины $X,Y$ называются \textbf{некорреллированными}, если $r(X, Y) = 0$. 

    Заметим, что если две случайные величины $X, Y$ независимы, то они некорреллированы. Обратное, вообще говоря, неверно.

    \subsection{Моменты распределений}
    Число $\mexp \xi^k$ называется \textbf{моментом порядка $k$} или \textbf{$k$-ым моментом случайной величины $\xi$}. 

    \part{Математическая статистика}
    \section{Оценки параметров}

    \subsection{Точечные оценки параметров}
    Основная задача математической статистики состоит в нахождении неизвестного распределения. Приведём пример: пусть в ящике $N$ шаров, $M$ из которых чёрные. Вынимаем $n$ шаров. Вопрос теории вероятностей: с какой вероятностью среди них $m$ чёрных шаров? Вопрос математической статистики: пусть среди $n$ шаров оказалось $m$ чёрных, сколько всего чёрных шаров в коробке (чему равно $M$)?

    Предположим, нам изначально известно, что неизвестное распределение принадлежит некоторому семейству распределений с функциями распределения $F_\theta, \theta \in \Theta \subset \RR^k$. Тогда задача сводится к нахождению неизвестного параметра $\theta_0 \in \Theta$, соответствующего нашему неизвестному распределению.

    \begin{definition*}[Выборка и статистики]
        Вектор $X = (X_1,\,\ldots,\,X_n)$ с независимыми компонентами, где каждая случайная величина имеет одно и то же распределение, называется \textbf{выборкой}.

        Все случайные велиичны вида $T(X)$ называются \textbf{статистиками}.
    \end{definition*}

    Проведя серию независимых экспериментов с неизвестным распределением, мы получаем выборку $X = (X_1,\,\ldots,\,X_n)$. По выборке нам бы хотелось определить значение $\widehat{\theta}_n$, которое в каком-то смысле близко к реальному неизвестному значению $\theta_0$. Тем самым, мы строим функцию от выборки $\widehat{\theta}_n(X)$ со значением во множестве параметров. Такие случайные величины называются оценками неизвестного параметра.

    \subsection{Интервальные оценки. Доверительные интервалы}
    Ясно, что информация о состоятельности оценки $\widehat{\theta}_n(X)$ не достаточно для того, чтобы что-то говорить о возможном значении параметра $\theta$. Предположим, что мы знаем ``скорость сходимости'', то есть для фиксированного $\alpha \in (0,\, 1)$, для фиксированного $\varepsilon > 0$ мы смогли подобрать номер $n$, для которого $\prob_\theta (\abs{\widehat{\theta}_n(X) - \theta} < \varepsilon) > 1 - \alpha$. Тогда параметр $\theta \in (\widehat{\theta}_n(X) - \varepsilon, \widehat{\theta}_n(X) + \varepsilon)$ с большой вероятностью.

    Обобщая данное наблюдение, приходим к определению доверительного интервала. Пусть заданы две статистики $\theta(X)$ и $\theta_2(X)$. Случайный интервал $(\theta_1(X), \theta_2(X))$ называется \textbf{доверительным интервалом} для параметра $\theta$ с уровнем доверия $1 - \alpha$, если при всех $\theta$ выполняется неравенство
    \begin{equation*}
        \prob_\theta(\theta_1^n(X) < \theta < \theta_2^n(X)) \geq 1 - \alpha.
    \end{equation*}
    Если же последовательность пар статистик $\theta_1^n(X)$ и $\theta_2^n(X)$, для которой
    \begin{equation*}
        \liminf_{n \to \infty} \prob_\theta(\theta_1^n(X) < \theta < \theta_2^n(X)) \geq 1 - \alpha,
    \end{equation*}
    то говорят, что задан асимптотический интервал уровня доверия $1 - \alpha$.
    \subsection{Свойства точечных оценок параметров}
    Сформулируем теперь, в каком смысле можно понимать близость оценки $\widehat{\theta}_n(X)$ к неизвестному параметру $\theta$.
    \begin{enumerate}[label=\Roman*.]
        \item \textbf{Несмещённость.} $\mexp_\theta \widehat{\theta}_n(X) = \theta$. Это естественное свойство, которое означает, что в среднем оценка даёт правильное значение неизвестного параметра.
        \item \textbf{Состоятельность.} $\lim_{n \to \infty} \widehat{\theta}_n(X) = \theta$ по вероятности $\prob_\theta$ (соответствующей функции распределения $F_\theta$). Обычно это свойство является следствием закона больших чисел и теорем непрерывности.
        \item \textbf{Сильная состоятельность.} $\lim_{n \to \infty} \widehat{\theta}_n(X) = \theta \prob_\theta$-п.н.
        \item \textbf{Асимптотическая нормальность.} Оценка $\widehat{\theta}_n(X)$ является асимптотически нормальной, если
        \begin{equation*}
            \sqrt{n}\left(\widehat{\theta}_n(X) - \theta\right) \to \ZZ \sim \mathcal{N\left(0, \sigma^2(\theta)\right)}
        \end{equation*}
        по распределению для некоторого числа $\sigma^2(\theta)$. Это условие влечёт состоятельность и позволяет оценивать вероятности событий $\alpha < \theta_n(X) < \beta$ с помощью нормального распределения.
        \item \textbf{Эффективность.} Несмещённая оценка $\widehat{\theta}_n(X)$ называется эффективной (в классе всех несмещённых оценок), если она имеет наименьшую дисперсию среди всех несмещённых оценок, т.е. $\mexp_\theta\left(\widehat{\theta}_n(X) - \theta\right)^2 \leq \mexp_\theta(\theta_n^*(X) - \theta)^2$ для произвольной несмещённой оценки $\theta_n^*(X)$. Это очень естественное требование, особенно, если принять во внимание неравенство Чебышева:
        \begin{equation*}
            \prob(\abs{\widehat{\theta}_n(X) - \theta} \geq \varepsilon) \leq \frac{\var\widehat{\theta}_n(X)}{\varepsilon^2}. 
        \end{equation*}
    \end{enumerate}
    
    \section{Метод максимального правдоподобия}
    \subsection{Метод максимального правдоподобия для оценки параметров распределений}
    \begin{definition*}[Обобщённая плотность]
        Пусть случайная величина $\xi$ имеет дискретное или непрерывное распределение. \textbf{Обобщённой плотностью} $\varrho$ этого распределения (этой случайной величины) назовём обычную плотность в случае, когда $\xi$ имеет непрерывное распределение, а в дискретном случае положим $\varrho(t) = \prob(\xi = t)$, где $t$ принадлежит области значений $\xi$.
    \end{definition*}
    \begin{definition*}[Функция правдоподобия]
        Пусть $X_1,\,\ldots,\,X_n$~--- выборка из распределения с обобщённой плотностью $\varrho_\theta$. Обобщённая плотность вектора $X = (X_1,\,\ldots,\,X_)$
        \begin{equation*}
            p(x, \theta) := \varrho_\theta(x_1) \cdot \ldots \cdot \varrho_\theta(x_n)
        \end{equation*}
        называется \textbf{функцией правдоподобия}, а её логарифм
        \begin{equation*}
            L(x,\,\theta) = \ln p(x,\,\theta) = \sum_{i=1}^n \ln \varrho_\theta(x_i)
        \end{equation*}
        называют \textbf{логарифмической функцией правдоподобия}.
    \end{definition*}
    \begin{definition*}[Оценка максимального правдоподобия]
        Пусть $X_1,\,\ldots,\,X_n$~--- выборка из распределения с обобщённой плотностью $\varrho_\theta$. Оценка $\widehat{\theta}_n(X)$ называется \textbf{оценкой максимального правдоподобия}, если при фиксированном $x = (x_1,\,\ldots,\, x_n)$ число $\widehat{\theta}_n(x)$ есть точка максимума функции правдоподобия $p(x, \theta)$. То есть, в качестве оценки выбирается такое значение $\theta$, при котором наблюдаемые значения $X = (X_1,\, \ldots,\, X_N)$ имеют максимальную вероятность.
    \end{definition*}
    \begin{comment*}
        Ясно, что точки максимума функций $p(x, \theta)$ и $L(x, \theta)$ совпадают. Обычно для поиска точки максимума просто приравнивают к нулю производную функции $L(x, \theta)$, так как считать производную функции для суммы проще, чем для произведения.
    \end{comment*}
    \begin{example*}
        Пусть $X_1,\, \ldots ,\, X_n$~--- выборка из распределения Бернулли с вероятностью успеха $\theta$. Найдём оценку по методу максимального правдоподобия. В нашем случае обобщённая плотность распределения имеет вид $\varrho_\theta(t) = \theta^t(1 - \theta)^{1-t}$, где $t \in \{0,\, 1\}$. Тем самым, функция правдоподобия 
        \begin{equation*}
            p(x,\, \theta) = \theta^{\sum_{j=1}^n x_j} (1 - \theta)^{n - \sum_{j=1}^n x_j}
        \end{equation*}
        и логарифмическая функция правдоподобия
        \begin{equation*}
            L(x,\, \theta) = \left(\sum_{j=1}^n x_j\right) \ln \theta + \left(n - \sum_{j=1}^n x_j\right) \ln(1 - \theta).
        \end{equation*}
        Дифференцируя, получаем уравнения
        \begin{equation*}
            \frac{\sum x_j}{\widehat{\theta}_n} - \frac{n - \sum x_j}{1 - \widehat{\theta}_n} = 0,
        \end{equation*}
        откуда $\widehat{\theta}_n = \frac{\sum x_j}{n} = \bar{X}_n$.

        Заметим, что в точке максимума $\widehat{\theta}_n(x)$ выполнено
        \begin{equation*}
            L(x, \theta) \approx L(x, \widehat{\theta}_n(x)) + \frac{1}{2}\delta_{\theta \theta}^2 L(x, \widehat{\theta}_n(x))(\theta - \widehat{\theta}_n(x))^2,
        \end{equation*}
        то есть вторая производная $\delta_{\theta \theta}^2 L(x, \theta)$ описывает сколь быстро логарифмическая функция правдоподобия меняется в окрестности точки максимума. Пусть теперь на самом деле выборка задана распределением с параметром $\theta_0$. Вычисляем среднее значение данной второй производной в точке $\theta_0$ со знаком минус (в дискретном случае все знаки интегралов надо заменить на знаки сумм):
        \begin{equation*}
            -\mexp_{\theta_0} \delta_{\theta \theta}^2 L(X, \theta_0) = -\int \left[\delta_{\theta \theta}^2 \ln p(x,\, \theta_0)\right] p(x, \theta_0) dx = \int \left[\frac{\delta_{\theta \theta}^2 p(x,\, \theta_0)}{p(x,\, \theta_0)} - \frac{(\delta_\theta p(x, \theta_0))^2}{p^2(x, \theta_0)}\right] p(x,\, \theta_0) dx.
        \end{equation*}
        Предположим, что для $p(x,\, \theta)$ выполнено следующее условие регулярности: $p(x, \theta)$ положительна, непрерывна и дифференцируема по $\theta$, причём операции дифференцирования и интегрирования перестановочны.

        Так как
        \begin{equation*}
            \int p(x, \theta) dx = 1,
        \end{equation*}
        то
        \begin{equation*}
            \int \delta_\theta p(x, \theta) dx = \int \delta_{\theta \theta}^2 p(x, \theta_0) dx = 0,
        \end{equation*}
        поэтому
        \begin{multline*}
            \mexp_{\theta_0} \delta_{\theta \theta}^2 L(X,\, \theta_0) = \int \frac{(\delta_\theta p(x, \theta_0))^2}{p(x, \theta_0)} dx = \int \frac{(\delta_\theta p(x, \theta_0))^2}{p^2(x, \theta_0)} p(x, \theta_0) dx =\\ = \int \left[\delta_\theta \ln p(x, \theta_0)\right]^2 p(x,\, \theta_0) dx = \mexp_{\theta_0}\left[\delta_\theta L(X,\, \theta_0)\right]^2.
        \end{multline*}
    \end{example*}

    \subsection{Свойства ММП-оценок}
    \begin{theorem*}[Состоятельность]
        Пусть $\theta \in (\alpha, \beta),\ \varrho(x)$ непрерывна по $\theta$ и при каждом $x$ функция $L(x, \theta)$ имеет ровно одну точку локального максимума $\theta_n(x)$ в $(\alpha, \beta)$. Тогда $\theta_n(X)$ сходится по вероятности к $\theta_0$ (то есть является состоятельной оценкой).
    \end{theorem*}
    \begin{theorem*}
        Пусть в дополнение к предположениям предыдущей теоремы выполнены условия регулярности $\varrho_\theta(x)$ положительна, трижды непрерывно дифференцируема по $\theta$, причём операции дифференцирования и интегрирования перестановочны и $\abs{\frac{\delta^3 \ln \varrho_\theta(x)}{\delta \theta^3}} \leq H(x)$, $ \mexp_\theta H(X_1) = \int H(x) \varrho \theta(x) dx \leq M$~--- не зависит от $\theta$.

        Тогда оценка, полученная по методу максимального правдоподобия, будет асимптотически нормальной с асимптотической дисперсией $\frac{1}{i(\theta)}$.
    \end{theorem*}

    \section{Центральная предельная теорема}
    Пусть $\{\xi\}$~--- последовательность независимых одинаково распределённых случайных величин, причём $\mexp \xi_1 = \mu$ и $\var\xi_1 = \sigma^2$. Тогда для всех $x$
    \begin{equation*}
        \lim_{n \to \infty} \prob\left(\frac{\xi_1 + \ldots + \xi_n - n\mu}{\sigma \sqrt{n}} \leq x\right) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^x e^{-y^2/2} dy.
    \end{equation*}

    \section{Закон больших чисел}
    \begin{theorem*}[Закон больших чисел]
        Предположим, что $\{\xi_n\}$~--- последовательность независимых одинаково распределённых случайных величин таких, что $\mexp \xi_n^4 < \infty$. Пусть $\mexp \xi_n = \mu$. Тогда с вероятностью единица
        \begin{equation*}
            \lim_{n \to \infty} \frac{\xi_1 + \ldots + \xi_n}{n} = \mu.
        \end{equation*}
    \end{theorem*}

    
\end{document}